{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "Wave2Web.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.10 64-bit ('swarm': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "14197652a433eb037d960e22bff016bdd324703c37019070c3a5d135b3893527"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "# google colab setup\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/s6/6mxc4sn90wq5byp13q5h4jkw0000gn/T/ipykernel_3374/1982705843.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# google colab setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/gdrive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "metadata": {
        "id": "alToowD8GMKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install fast-ml\n",
        "# !CMDSTAN=/tmp/cmdstan-2.22.1 STAN_BACKEND=CMDSTANPY pip install prophet\n",
        "# !pip install pystan==2.19.1.1\n",
        "# !pip install prophet"
      ],
      "outputs": [],
      "metadata": {
        "id": "bueOZIFdIBfF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn import ensemble, metrics\n",
        "import matplotlib.pyplot as plt\n",
        "from fast_ml import model_development as md\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# from prophet import Prophet\n",
        "# from prophet.plot import plot_plotly, plot_components_plotly\n",
        "# from prophet.diagnostics import cross_validation\n",
        "# from prophet.plot import plot_cross_validation_metric"
      ],
      "outputs": [],
      "metadata": {
        "id": "DC-zRhnwGsuC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# use_tpu = True\n",
        "\n",
        "# if use_tpu:\n",
        "#     assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'\n",
        "\n",
        "# if 'COLAB_TPU_ADDR' in os.environ:\n",
        "#   TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])\n",
        "# else:\n",
        "#   TF_MASTER=''\n",
        "\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)\n",
        "# tf.config.experimental_connect_to_cluster(resolver)\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "outputs": [],
      "metadata": {
        "id": "ClRXIaETXXBK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# pathing\n",
        "project_dir = Path(\"gdrive/MyDrive/Assistance/Shivam/teamPoseidon\")\n",
        "\n",
        "data_file = project_dir / \"final_dataset_csv.csv\""
      ],
      "outputs": [],
      "metadata": {
        "id": "upXIev7sGx8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Hypers\n",
        "SPLITS = {\n",
        "    \"train\":0.7,\n",
        "    \"validation\":0.2,\n",
        "    \"test\":0.1,\n",
        "}\n",
        "\n",
        "# they definitely needs optimization.\n",
        "BATCH_SIZE = 64 # bacth size in batch-SGD/variants\n",
        "BUFFER_SIZE = 10 # for shuffling the dataset\n",
        "STEP = 1 # for creation of dataset\n",
        "\n",
        "# Train and evaluate\n",
        "STEPS_PER_EPOCH = 10 # hyperparameter\n",
        "EPOCHS = 100 # hyperparameter\n",
        "\n",
        "FORECAST_LENGTH = 90\n",
        "\n",
        "HISTORY_LENGTH = 10\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "6VIK9gs3HXOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# data loading\n",
        "df = pd.read_csv(data_file, sep=',', na_values=[\" \", \"&nbsp;\"], parse_dates=[\"FLOW_DATE\"], header=0)\n",
        "df = df.ffill()\n",
        "df = df.set_index(\"FLOW_DATE\")\n",
        "df[\"dummy\"] = 1"
      ],
      "outputs": [],
      "metadata": {
        "id": "CRMspbgDHLX9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "df[\"RES_LEVEL_FT\"]"
      ],
      "outputs": [],
      "metadata": {
        "id": "H_M6TqdaVMNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# prediction_df = df.copy()\n",
        "# prediction_df[\"ds\"] = prediction_df.index\n",
        "# prediction_df['y'] = prediction_df['RES_LEVEL_FT']\n",
        "# prediction_df[\"cap\"] = prediction_df[\"RES_LEVEL_FT\"].max()\n",
        "\n",
        "# model = Prophet(\n",
        "#     growth=\"flat\",\n",
        "#     seasonality_mode=\"multiplicative\",\n",
        "#     daily_seasonality=False,\n",
        "#     weekly_seasonality=False,\n",
        "#     changepoint_prior_scale=0.01\n",
        "# )\n",
        "\n",
        "# model.fit(prediction_df)\n",
        "\n",
        "# future = model.make_future_dataframe(periods=3*365)\n",
        "# forecast = model.predict(future)\n",
        "\n",
        "# forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
      ],
      "outputs": [],
      "metadata": {
        "id": "ky0m1F3R0g8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plot_plotly(model, forecast)"
      ],
      "outputs": [],
      "metadata": {
        "id": "a__AMHKn06oD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# forecast"
      ],
      "outputs": [],
      "metadata": {
        "id": "2-pEvUtI183i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plot_components_plotly(model, forecast)"
      ],
      "outputs": [],
      "metadata": {
        "id": "cLuZEFgW0_Ay"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# df_cv = cross_validation(model, initial='730 days', period='365 days', horizon = '90 days')"
      ],
      "outputs": [],
      "metadata": {
        "id": "NmaSnRQU1Bb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# ## Add cross validation\n",
        "# fig = plot_cross_validation_metric(df_cv, metric='mse')"
      ],
      "outputs": [],
      "metadata": {
        "id": "1iggbls_08t7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "used_cols = ['PRESENT_STORAGE_TMC', 'INFLOW_CUSECS', 'OUTFLOW_CUECS', 'tempC', 'windspeedKmph', 'precipMM', 'humidity', 'pressure (mB)', 'cloudcover (%)', 'HeatIndexC', 'DewPointC', 'WindChillC', 'WindGustKmph', 'RES_LEVEL_FT']\n",
        "target_col = 'RES_LEVEL_FT'\n",
        "\n",
        "# X_train, y_train, X_valid, y_valid, X_test, y_test = md.train_valid_test_split(\n",
        "#     df[used_cols + [\"dummy\"]],\n",
        "#     target=\"dummy\", # use dummy for train test split before preproccessing\n",
        "#     train_size=SPLITS[\"train\"],\n",
        "#     valid_size=SPLITS[\"validation\"],\n",
        "#     test_size=SPLITS[\"test\"],\n",
        "#     method=\"sorted\",\n",
        "#     sort_by_col=\"FLOW_DATE\",\n",
        "# )\n",
        "\n",
        "# data_dict = {\n",
        "#     \"train\": X_train,\n",
        "#     \"validation\": X_valid,\n",
        "#     \"test\": X_test,\n",
        "# }"
      ],
      "outputs": [],
      "metadata": {
        "id": "iFfQvAd9HmXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# windowed_ys = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "#     data=data_dict[\"train\"][target_col],\n",
        "#     targets=None,\n",
        "#     sequence_length=FORECAST_LENGTH,\n",
        "#     sequence_stride=1,\n",
        "#     shuffle=False,\n",
        "#     batch_size=1,\n",
        "# )"
      ],
      "outputs": [],
      "metadata": {
        "id": "uR83s7DrTjxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def get_windowed_dataset(\n",
        "    dataframe,\n",
        "    target_col,\n",
        "    history_length=HISTORY_LENGTH,\n",
        "    forecast_length=FORECAST_LENGTH,\n",
        "    batch_size=32,\n",
        "    stride=30,\n",
        "    ):\n",
        "\n",
        "    data = dataframe.values.copy()\n",
        "    ys = dataframe[target_col]\n",
        "\n",
        "    data_future = data.copy()\n",
        "    # print(data_future.shape)\n",
        "    data = data[:-forecast_length-history_length]\n",
        "    # print(data.shape)\n",
        "    ys = ys[forecast_length+history_length:] # predict window starting at next time step\n",
        "\n",
        "    # window the labels to make forecasts in the future\n",
        "    windowed_ys = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=ys,\n",
        "        targets=None, # ys.index,\n",
        "        sequence_length=forecast_length,\n",
        "        sequence_stride=stride,\n",
        "        shuffle=False,\n",
        "        batch_size=1,\n",
        "    )\n",
        "    print(data.shape)\n",
        "    ys = np.array(list(windowed_ys.as_numpy_iterator())).squeeze()\n",
        "    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=data,\n",
        "        targets=ys,\n",
        "        sequence_length=history_length,\n",
        "        sequence_stride=stride,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    print(data_future.shape)\n",
        "    future_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "        data=data_future,\n",
        "        targets=np.ones((data_future.shape[0],1)),\n",
        "        sequence_length=history_length,\n",
        "        sequence_stride=stride,\n",
        "        shuffle=False,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    return dataset, future_dataset"
      ],
      "outputs": [],
      "metadata": {
        "id": "H9AbNkN-CQRC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def dataset_to_arrays(dataset, flatten=True, last_y=True):\n",
        "    all_xs = []\n",
        "    all_ys = []\n",
        "    for x, y in dataset.as_numpy_iterator():\n",
        "        all_xs.append(x)\n",
        "        all_ys.append(y)\n",
        "    ys = np.concatenate(all_ys)\n",
        "    xs = np.concatenate(all_xs)\n",
        "    if flatten:\n",
        "        xs = xs.reshape(xs.shape[0], -1)\n",
        "    if last_y:\n",
        "        ys = ys[:, -1]\n",
        "    return xs, ys"
      ],
      "outputs": [],
      "metadata": {
        "id": "nDZVbLnuj0D8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "historical_dataset, future_dataset  = get_windowed_dataset(\n",
        "    df[used_cols],# + [\"dummy\"]],\n",
        "    target_col=target_col,\n",
        "    stride=1\n",
        "    )\n",
        "\n",
        "historical_xs, historical_ys = dataset_to_arrays(historical_dataset)\n",
        "future_xs, _ = dataset_to_arrays(future_dataset)\n",
        "print(historical_xs.shape, historical_ys.shape, future_xs.shape)\n",
        "\n",
        "data_df = pd.DataFrame(np.concatenate([np.arange(historical_ys.shape[0])[:, None], historical_xs, historical_ys[:,None]], axis=-1))\n",
        "\n",
        "X_train, y_train, X_valid, y_valid, X_test, y_test = md.train_valid_test_split(\n",
        "    data_df,\n",
        "    target=data_df.columns[-1], # use dummy for train test split before preproccessing\n",
        "    train_size=SPLITS[\"train\"],\n",
        "    valid_size=SPLITS[\"validation\"],\n",
        "    test_size=SPLITS[\"test\"],\n",
        "    method=\"sorted\",\n",
        "    sort_by_col=data_df.columns[0],\n",
        ")\n",
        "\n",
        "# use .iloc[:,1:]to drop first column\n",
        "data_dict = {\n",
        "    \"train\": {\"X\":X_train.iloc[:,1:], \"y\":y_train},\n",
        "    \"validation\": {\"X\":X_valid.iloc[:,1:], \"y\":y_valid},\n",
        "    \"test\": {\"X\":X_test.iloc[:,1:], \"y\":y_test},\n",
        "}\n",
        "\n",
        "X_means = data_dict[\"train\"][\"X\"].mean()\n",
        "X_stdev = data_dict[\"train\"][\"X\"].std()\n",
        "y_means = data_dict[\"train\"][\"y\"].mean()\n",
        "y_stdev = data_dict[\"train\"][\"y\"].std()\n",
        "for partition in data_dict.keys():\n",
        "    data_dict[partition][\"X\"] = (data_dict[partition][\"X\"] - X_means) / X_stdev\n",
        "    data_dict[partition][\"y\"] = (data_dict[partition][\"y\"] - y_means) / y_stdev\n",
        "\n",
        "future_xs = (future_xs - X_means[None,]) / X_stdev[None,:]\n",
        "# future_xs = (future_xs - X_means[None, :140]) / X_stdev[None, :140]"
      ],
      "outputs": [],
      "metadata": {
        "id": "s0HJUPs8kAuX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# dataset_dict = {\n",
        "#     partition_name: get_windowed_dataset(partition_data, target_col=target_col)\n",
        "#     for partition_name, partition_data in data_dict.items()\n",
        "# }\n",
        "# future_dataset_dict = {k:v[1] for k,v in dataset_dict.items()}\n",
        "# dataset_dict = {k:v[0] for k,v in dataset_dict.items()}"
      ],
      "outputs": [],
      "metadata": {
        "id": "i5Rh1QzKDsqE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# x_example,y_example = next(dataset_dict[\"train\"].take(1).as_numpy_iterator())\n",
        "# print(\"X shape\", x_example.shape)\n",
        "# print(\"y shape\", y_example.shape)"
      ],
      "outputs": [],
      "metadata": {
        "id": "D7xXA_PQNcda"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# lstm training\n",
        "# with strategy.scope():\n",
        "\n",
        "#     multi_step_model = tf.keras.models.Sequential()\n",
        "#     # multi_step_model.add(tf.keras.layers.Conv1D(\n",
        "#     #     filters=32,\n",
        "#     #     kernel_size=30,\n",
        "#     #     input_shape=x_example.shape[-2:],\n",
        "#     #     ))\n",
        "#     # # multi_step_model.add(tf.keras.layers.BatchNormalization())\n",
        "#     # multi_step_model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "#     # multi_step_model.add(tf.keras.layers.BatchNormalization())\n",
        "#     # multi_step_model.add(tf.keras.layers.Conv1D(\n",
        "#     #     filters=32,\n",
        "#     #     kernel_size=30,\n",
        "#     #     )\n",
        "#     # )\n",
        "#     # multi_step_model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "#     # multi_step_model.add(tf.keras.layers.BatchNormalization())\n",
        "#     # multi_step_model.add(tf.keras.layers.Conv1D(\n",
        "#     #     filters=32,\n",
        "#     #     kernel_size=30,\n",
        "#     #     )\n",
        "#     # )\n",
        "#     # multi_step_model.add(tf.keras.layers.LeakyReLU())\n",
        "#     # multi_step_model.add(tf.keras.layers.Flatten())\n",
        "#     multi_step_model.add(tf.keras.layers.LSTM(\n",
        "#         32,\n",
        "#         return_sequences=True,\n",
        "#         input_shape=x_example.shape[-2:],\n",
        "#         # activation=\"relu\",\n",
        "#         ),\n",
        "#     )\n",
        "#     multi_step_model.add(tf.keras.layers.LSTM(\n",
        "#         8, \n",
        "#         activation='relu',\n",
        "#         )\n",
        "#     )\n",
        "#     multi_step_model.add(tf.keras.layers.Dense(90)) # for 90 outputs\n",
        "#     # multi_step_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))\n",
        "#     multi_step_model.add(tf.keras.layers.Dense(FORECAST_LENGTH)) # for 90 outputs\n",
        "\n",
        "#     # multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
        "#     # multi_step_model.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=1.0), loss='mae')\n",
        "#     multi_step_model.compile(\n",
        "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.1, clipvalue=1.0),\n",
        "#         loss='mae'\n",
        "#         )\n",
        "# early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss',\n",
        "#     patience=5,\n",
        "#     restore_best_weights=True\n",
        "#     )\n",
        "\n",
        "# multi_step_history = multi_step_model.fit(\n",
        "#     dataset_dict[\"train\"].cache().shuffle(BUFFER_SIZE).repeat(),\n",
        "#     epochs=EPOCHS,\n",
        "#     steps_per_epoch=STEPS_PER_EPOCH,\n",
        "#     validation_data=dataset_dict[\"validation\"],\n",
        "#     validation_steps=1,\n",
        "#     callbacks=[early_stopping_cb],\n",
        "#     )"
      ],
      "outputs": [],
      "metadata": {
        "id": "1RwkFd-FNPSR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# # Plot train and validation loss over epochs\n",
        "# def plot_train_history(history, title):\n",
        "#     loss = history.history['loss']\n",
        "#     val_loss = history.history['val_loss']\n",
        "#     epochs = range(len(loss))\n",
        "    \n",
        "#     plt.figure()\n",
        "\n",
        "#     plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "#     plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "#     plt.title(title)\n",
        "#     plt.legend()\n",
        "#     plt.grid()\n",
        "\n",
        "#     plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "OcU1j3e-OZW9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# plot_train_history(multi_step_history, \"lstm\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "4vbGPVOpPZo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def create_time_steps(length):\n",
        "  return list(range(-length, 0))\n",
        "\n",
        "#plotting function\n",
        "def multi_step_plot(history, true_future, prediction, title=\"\"):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "  plt.grid()\n",
        "  plt.plot(num_in, np.array(history), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "  \n"
      ],
      "outputs": [],
      "metadata": {
        "id": "ZeNTDv0xUIDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# partition_name = \"train\"\n",
        "# y_pred = multi_step_model.predict_generator(dataset_dict[partition_name])\n",
        "\n",
        "# all_xs = []\n",
        "# all_ys = []\n",
        "# for x,y in dataset_dict[partition_name].as_numpy_iterator():\n",
        "#     all_xs.append(x)\n",
        "#     all_ys.append(y)\n",
        "# y_true = np.concatenate(all_ys)\n",
        "# xs = np.concatenate(all_xs)\n",
        "\n",
        "# print(\"shapes\", xs.shape, y_true.shape, y_pred.shape)\n",
        "\n",
        "# for i in range(0, xs.shape[0],xs.shape[0]//2):\n",
        "#     multi_step_plot(xs[i, :,-1], y_true[i], y_pred[i], title=f\"{partition_name} offset {i}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "cLTSk9PwMiki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# partition_name = \"validation\"\n",
        "# y_pred = multi_step_model.predict_generator(dataset_dict[partition_name])\n",
        "\n",
        "# all_xs = []\n",
        "# all_ys = []\n",
        "# for x,y in dataset_dict[partition_name].as_numpy_iterator():\n",
        "#     all_xs.append(x)\n",
        "#     all_ys.append(y)\n",
        "# y_true = np.concatenate(all_ys)\n",
        "# xs = np.concatenate(all_xs)\n",
        "\n",
        "# print(\"shapes\", xs.shape, y_true.shape, y_pred.shape)\n",
        "\n",
        "# for i in range(0, xs.shape[0],xs.shape[0]//2):\n",
        "#     multi_step_plot(xs[i, :,-1], y_true[i], y_pred[i], title=f\"{partition_name} offset {i}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "DK91Af_2bEES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# partition_name = \"test\"\n",
        "# y_pred = multi_step_model.predict_generator(dataset_dict[partition_name])\n",
        "\n",
        "# all_xs = []\n",
        "# all_ys = []\n",
        "# for x,y in dataset_dict[partition_name].as_numpy_iterator():\n",
        "#     all_xs.append(x)\n",
        "#     all_ys.append(y)\n",
        "# y_true = np.concatenate(all_ys)\n",
        "# xs = np.concatenate(all_xs)\n",
        "\n",
        "# print(\"shapes\", xs.shape, y_true.shape, y_pred.shape)\n",
        "\n",
        "# for i in range(0, xs.shape[0],xs.shape[0]//2):\n",
        "#     multi_step_plot(xs[i, :,-1], y_true[i], y_pred[i], title=f\"{partition_name} offset {i}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fy3YrMAYZKlX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# sklearn_datasets_dict = {\n",
        "#     k:dataset_to_arrays(v)\n",
        "#     for k,v in dataset_dict.items()\n",
        "# }"
      ],
      "outputs": [],
      "metadata": {
        "id": "S2QgGTNk-TQf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# for k, (xs, ys) in sklearn_datasets_dict.items():\n",
        "#     print(f\"{k}:{xs.shape}, {ys.shape}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "uI_zjC8g-ToK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# model = ensemble.BaggingRegressor(n_estimators=50)\n",
        "# model.fit(sklearn_datasets_dict[\"train\"][0], sklearn_datasets_dict[\"train\"][1])\n",
        "\n",
        "kwargs = dict(\n",
        "    max_depth=5,\n",
        "    subsample=0.5,\n",
        ")\n",
        "\n",
        "# Each model has to be separate\n",
        "# lower_model = ensemble.GradientBoostingRegressor(loss=\"quantile\",alpha=0.1,**kwargs)\n",
        "# mid_model = ensemble.GradientBoostingRegressor(loss=\"ls\", **kwargs)\n",
        "# upper_model = ensemble.GradientBoostingRegressor(loss=\"quantile\", alpha=0.9, **kwargs)\n",
        "\n",
        "lower_model = ensemble.GradientBoostingRegressor(loss=\"quantile\",alpha=0.1,**kwargs,)\n",
        "mid_model = ensemble.GradientBoostingRegressor(loss=\"ls\", **kwargs)\n",
        "upper_model = ensemble.GradientBoostingRegressor(loss=\"quantile\", alpha=0.9, **kwargs)\n",
        "\n",
        "lower_model.fit(data_dict[\"train\"]['X'], data_dict[\"train\"][\"y\"])\n",
        "mid_model.fit(data_dict[\"train\"][\"X\"], data_dict[\"train\"][\"y\"])\n",
        "upper_model.fit(data_dict[\"train\"][\"X\"], data_dict[\"train\"][\"y\"])\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "C5V7RQA6_Zky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def rescale_y(y):\n",
        "    # return y\n",
        "    return y*y_stdev+y_means"
      ],
      "outputs": [],
      "metadata": {
        "id": "K_ObnO6vYfnY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "result_dict = {}\n",
        "for partition, d in data_dict.items():\n",
        "    y_pred = mid_model.predict(d[\"X\"])\n",
        "    print(f\"{partition} mae:\", metrics.mean_absolute_error(rescale_y(d[\"y\"]), rescale_y(y_pred)))\n",
        "    # multi_step_plot(xs[:, -1], ys, y_pred, title=f\"{partition}\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "J9i9YELy_mR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "partition_name = \"train\"\n",
        "\n",
        "xs = data_dict[partition_name][\"X\"]\n",
        "ys = rescale_y(data_dict[partition_name][\"y\"])\n",
        "ypred_lb = rescale_y(lower_model.predict(xs))\n",
        "ypred_mid = rescale_y(mid_model.predict(xs))\n",
        "ypred_ub = rescale_y(upper_model.predict(xs))\n",
        "\n",
        "num_past = xs.shape[0]\n",
        "\n",
        "num_future = ypred_lb.shape[0]\n",
        "\n",
        "x_indices_future = np.arange(num_future)\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# plot future predictions\n",
        "plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)\n",
        "plt.plot(x_indices_future, ypred_mid, \"--\")\n",
        "plt.plot(x_indices_future, ys)\n",
        "\n",
        "plt.title(partition_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "XZLgG17CLgef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "partition_name = \"validation\"\n",
        "\n",
        "xs = data_dict[partition_name][\"X\"]\n",
        "ys = rescale_y(data_dict[partition_name][\"y\"])\n",
        "ypred_lb = rescale_y(lower_model.predict(xs))\n",
        "ypred_mid = rescale_y(mid_model.predict(xs))\n",
        "ypred_ub = rescale_y(upper_model.predict(xs))\n",
        "\n",
        "num_past = xs.shape[0]\n",
        "\n",
        "num_future = ypred_lb.shape[0]\n",
        "\n",
        "x_indices_future = np.arange(num_future)\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# plot future predictions\n",
        "plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)\n",
        "plt.plot(x_indices_future, ypred_mid, \"--\")\n",
        "plt.plot(x_indices_future, ys)\n",
        "\n",
        "plt.title(partition_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UMhA9oC-E3rG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "partition_name = \"test\"\n",
        "\n",
        "xs = data_dict[partition_name][\"X\"]\n",
        "ys = rescale_y(data_dict[partition_name][\"y\"])\n",
        "ypred_lb = rescale_y(lower_model.predict(xs))\n",
        "ypred_mid = rescale_y(mid_model.predict(xs))\n",
        "ypred_ub = rescale_y(upper_model.predict(xs))\n",
        "\n",
        "num_past = xs.shape[0]\n",
        "\n",
        "num_future = ypred_lb.shape[0]\n",
        "\n",
        "x_indices_future = np.arange(num_future)\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# plot future predictions\n",
        "plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)\n",
        "plt.plot(x_indices_future, ypred_mid, \"--\")\n",
        "plt.plot(x_indices_future, ys)\n",
        "\n",
        "plt.title(partition_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "RTtJzJWtHcFQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# # xgboost\n",
        "# train mae: 0.09998867638561532\n",
        "# validation mae: 0.5098038188379816\n",
        "# test mae: 0.6213932867880211\n"
      ],
      "outputs": [],
      "metadata": {
        "id": "u6X2RAl-Lii2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "X_means.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "zdHNiwfinApj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "\n",
        "\n",
        "ypred_lb = rescale_y(lower_model.predict(xs))\n",
        "ypred_mid = rescale_y(mid_model.predict(xs))\n",
        "ypred_ub = rescale_y(upper_model.predict(xs))\n",
        "\n",
        "num_past = xs.shape[0]\n",
        "\n",
        "num_future = ypred_lb.shape[0]\n",
        "\n",
        "x_indices_future = np.arange(num_future)\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# plot future predictions\n",
        "plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)\n",
        "plt.plot(x_indices_future, ypred_mid, \"--\")\n",
        "plt.plot(x_indices_future, ys)\n",
        "\n",
        "plt.title(partition_name)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qdaMuyjbTNTu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "historical_xs.shape, X_means.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "_4cHE3fnadq7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# refit model on full historical dataset and predict on future dataset\n",
        "\n",
        "historical_training_xs = (historical_xs - X_means[None,]) / X_stdev[None,]\n",
        "historical_training_ys = (historical_ys - y_means) / y_stdev\n",
        "\n",
        "lower_model = ensemble.GradientBoostingRegressor(loss=\"quantile\",alpha=0.1,**kwargs,)\n",
        "mid_model = ensemble.GradientBoostingRegressor(loss=\"ls\", **kwargs)\n",
        "upper_model = ensemble.GradientBoostingRegressor(loss=\"quantile\", alpha=0.9, **kwargs)\n",
        "\n",
        "lower_model.fit(historical_training_xs, historical_training_ys)\n",
        "mid_model.fit(historical_training_xs, historical_training_ys)\n",
        "upper_model.fit(historical_training_xs, historical_training_ys)\n",
        "\n",
        "\n",
        "partition_name = \"future\"\n",
        "\n",
        "# d = data_dict[partition_name]\n",
        "xs = future_xs[-90:]\n",
        "\n",
        "ypred_lb = rescale_y(lower_model.predict(xs))\n",
        "ypred_mid = rescale_y(mid_model.predict(xs))\n",
        "ypred_ub = rescale_y(upper_model.predict(xs))\n",
        "\n",
        "num_past = xs.shape[0]\n",
        "\n",
        "num_future = ypred_lb.shape[0]\n",
        "\n",
        "x_indices_future = np.arange(num_future)\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "# plot future predictions\n",
        "plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1, label = \"Predicted Lower/Upper Bound\")\n",
        "plt.plot(x_indices_future, ypred_mid, \"--\", label = \"Predicted Value\")\n",
        "# plt.plot(x_indices_future, ys)\n",
        "\n",
        "plt.title(\"Future Forecasts\", fontdict = {'fontsize' : 26})\n",
        "plt.style.use('tableau-colorblind10')\n",
        "plt.legend(prop={\"size\":20})\n",
        "plt.grid(True)\n",
        "plt.savefig(\"future_forecast.png\", dpi = 1200)"
      ],
      "outputs": [],
      "metadata": {
        "id": "KtXek20Hm3e2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "data_to_dump = {\n",
        "    \"mid\":ypred_mid,\n",
        "    \"lb\": ypred_lb,\n",
        "    \"ub\":ypred_ub,\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "Zmh_IcFcQUvF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.DataFrame(data_to_dump).to_csv(\"future_predictions.csv\", index=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "8m53hNRZU2NE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "df.index.shape"
      ],
      "outputs": [],
      "metadata": {
        "id": "m5Y3_rtfUqDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "pd.to_timedelta(FORECAST_LENGTH, unit=\"day\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "foft6F_4cSYP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "feature_names = [f\"{c} d-{i}\" for i in range(HISTORY_LENGTH-1, -1, -1) for c in used_cols]\n",
        "feature_importance_df = pd.DataFrame({\"feature\":feature_names, \"importance\":mid_model.feature_importances_})"
      ],
      "outputs": [],
      "metadata": {
        "id": "LX2zUK8mcMtI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(feature_importance_df.sort_values(\"importance\", ascending=False).head(10))"
      ],
      "outputs": [],
      "metadata": {
        "id": "5ZPapPrQeQbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [],
      "outputs": [],
      "metadata": {
        "id": "Ad7NRzr7e9lM"
      }
    }
  ]
}