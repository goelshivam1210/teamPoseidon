# -*- coding: utf-8 -*-
"""Copy of Wave2Web.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189atH0t_KfpmIlWrniOuNK7Chakv0xPV
"""

# google colab setup
# from google.colab import drive
# drive.mount("/content/gdrive")

# !pip install fast-ml
# !CMDSTAN=/tmp/cmdstan-2.22.1 STAN_BACKEND=CMDSTANPY pip install prophet
# !pip install pystan==2.19.1.1
# !pip install prophet

# imports
import os
from pathlib import Path

from sklearn import ensemble, metrics
import matplotlib.pyplot as plt
from fast_ml import model_development as md
import pandas as pd
import tensorflow as tf
import numpy as np

# from prophet import Prophet
# from prophet.plot import plot_plotly, plot_components_plotly
# from prophet.diagnostics import cross_validation
# from prophet.plot import plot_cross_validation_metric

# use_tpu = True

# if use_tpu:
    # assert 'COLAB_TPU_ADDR' in os.environ, 'Missing TPU; did you request a TPU in Notebook Settings?'

# if 'COLAB_TPU_ADDR' in os.environ:
#   TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])
# else:
#   TF_MASTER=''

# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(TF_MASTER)
# tf.config.experimental_connect_to_cluster(resolver)
# tf.tpu.experimental.initialize_tpu_system(resolver)
# strategy = tf.distribute.experimental.TPUStrategy(resolver)

# pathing
# project_dir = Path("/content/drive/MyDrive/teamPoseidon/dataset")

# data_file = project_dir / "final_dataset_csv.csv"
# print (data_file)
data_file = "final_dataset_csv.csv"
# from google.colab import drive
# drive.mount('/content/drive')

# Hypers
# do the split according to the numbers of the dates and the test samples.
SPLITS = {
    "train":0.6,
    "validation":0.2,
    "test":0.2,
}

# they definitely needs optimization.
BATCH_SIZE = 64 # bacth size in batch-SGD/variants
BUFFER_SIZE = 10 # for shuffling the dataset
STEP = 1 # for creation of dataset

# Train and evaluate
STEPS_PER_EPOCH = 10 # hyperparameter
EPOCHS = 100 # hyperparameter

FORECAST_LENGTH = 90

HISTORY_LENGTH = 10

# data loading
df = pd.read_csv(data_file, sep=',', na_values=[" ", "&nbsp;"], parse_dates=["FLOW_DATE"], header=0)
df = df.ffill()
df = df.set_index("FLOW_DATE")
df["dummy"] = 1

# prediction_df = df.copy()
# prediction_df["ds"] = prediction_df.index
# prediction_df['y'] = prediction_df['RES_LEVEL_FT']
# prediction_df["cap"] = prediction_df["RES_LEVEL_FT"].max()

# model = Prophet(
#     growth="flat",
#     seasonality_mode="multiplicative",
#     daily_seasonality=False,
#     weekly_seasonality=False,
#     changepoint_prior_scale=0.01
# )

# model.fit(prediction_df)

# future = model.make_future_dataframe(periods=3*365)
# forecast = model.predict(future)

# forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

# plot_plotly(model, forecast)

# forecast

# plot_components_plotly(model, forecast)

# df_cv = cross_validation(model, initial='730 days', period='365 days', horizon = '90 days')

# ## Add cross validation
# fig = plot_cross_validation_metric(df_cv, metric='mse')

used_cols = ['PRESENT_STORAGE_TMC', 'INFLOW_CUSECS', 'OUTFLOW_CUECS', 'tempC', 'windspeedKmph', 'precipMM', 'humidity', 'pressure (mB)', 'cloudcover (%)', 'HeatIndexC', 'DewPointC', 'WindChillC', 'WindGustKmph', 'RES_LEVEL_FT']
target_col = 'RES_LEVEL_FT'

X_train, y_train, X_valid, y_valid, X_test, y_test = md.train_valid_test_split(
    df[used_cols + ["dummy"]],
    target="dummy", # use dummy for train test split before preproccessing
    train_size=SPLITS["train"],
    valid_size=SPLITS["validation"],
    test_size=SPLITS["test"],
    method="sorted",
    sort_by_col="FLOW_DATE",
)

data_dict = {
    "train": X_train,
    "validation": X_valid,
    "test": X_test,
}

X_means = data_dict["train"].mean()
X_stdev = data_dict["train"].std()

for partition in data_dict.keys():
    data_dict[partition] = (data_dict[partition] - X_means) / X_stdev

# windowed_ys = tf.keras.preprocessing.timeseries_dataset_from_array(
#     data=data_dict["train"][target_col],
#     targets=None,
#     sequence_length=FORECAST_LENGTH,
#     sequence_stride=1,
#     shuffle=False,
#     batch_size=1,
# )

def get_windowed_dataset(
    dataframe,
    target_col,
    history_length=HISTORY_LENGTH,
    forecast_length=FORECAST_LENGTH,
    batch_size=32,
    stride=30,
    ):

    data = dataframe.values
    ys = dataframe[target_col]


    data = data[:-forecast_length-history_length]
    ys = ys[history_length:] # predict window starting at next time step

    # window the labels to make forecasts in the future
    windowed_ys = tf.keras.preprocessing.timeseries_dataset_from_array(
        data=ys,
        targets=None, # ys.index,
        sequence_length=forecast_length,
        sequence_stride=1,
        shuffle=False,
        batch_size=1,
    )

    ys = np.array(list(windowed_ys.as_numpy_iterator())).squeeze()
    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(
        data=data,
        targets=ys,
        sequence_length=history_length,
        sequence_stride=1,
        shuffle=False,
        batch_size=batch_size,
    )
    return dataset

dataset_dict = {
    partition_name: get_windowed_dataset(partition_data, target_col=target_col)
    for partition_name, partition_data in data_dict.items()
}

# x_example,y_example = next(dataset_dict["train"].take(1).as_numpy_iterator())
# print("X shape", x_example.shape)
# print("y shape", y_example.shape)

# with strategy.scope():

#     multi_step_model = tf.keras.models.Sequential()
#     # multi_step_model.add(tf.keras.layers.Conv1D(
#     #     filters=32,
#     #     kernel_size=30,
#     #     input_shape=x_example.shape[-2:],
#     #     ))
#     # # multi_step_model.add(tf.keras.layers.BatchNormalization())
#     # multi_step_model.add(tf.keras.layers.LeakyReLU())

#     # multi_step_model.add(tf.keras.layers.BatchNormalization())
#     # multi_step_model.add(tf.keras.layers.Conv1D(
#     #     filters=32,
#     #     kernel_size=30,
#     #     )
#     # )
#     # multi_step_model.add(tf.keras.layers.LeakyReLU())

#     # multi_step_model.add(tf.keras.layers.BatchNormalization())
#     # multi_step_model.add(tf.keras.layers.Conv1D(
#     #     filters=32,
#     #     kernel_size=30,
#     #     )
#     # )
#     # multi_step_model.add(tf.keras.layers.LeakyReLU())
#     # multi_step_model.add(tf.keras.layers.Flatten())
#     multi_step_model.add(tf.keras.layers.LSTM(
#         32,
#         return_sequences=True,
#         input_shape=x_example.shape[-2:],
#         # activation="relu",
#         ),
#     )
#     multi_step_model.add(tf.keras.layers.LSTM(
#         8, 
#         activation='relu',
#         )
#     )
#     multi_step_model.add(tf.keras.layers.Dense(90)) # for 90 outputs
#     # multi_step_model.add(tf.keras.layers.LeakyReLU(alpha=0.05))
#     multi_step_model.add(tf.keras.layers.Dense(FORECAST_LENGTH)) # for 90 outputs

#     # multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')
#     # multi_step_model.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=1.0), loss='mae')
#     multi_step_model.compile(
#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.1, clipvalue=1.0),
#         loss='mae'
#         )
# early_stopping_cb = tf.keras.callbacks.EarlyStopping(
#     monitor='val_loss',
#     patience=5,
#     restore_best_weights=True
#     )

# multi_step_history = multi_step_model.fit(
#     dataset_dict["train"].cache().shuffle(BUFFER_SIZE).repeat(),
#     epochs=EPOCHS,
#     steps_per_epoch=STEPS_PER_EPOCH,
#     validation_data=dataset_dict["validation"],
#     validation_steps=1,
#     callbacks=[early_stopping_cb],
#     )

# Plot train and validation loss over epochs
def plot_train_history(history, title):
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(loss))
    
    plt.figure()

    plt.plot(epochs, loss, 'b', label='Training loss')
    plt.plot(epochs, val_loss, 'r', label='Validation loss')
    plt.title(title)
    plt.legend()
    plt.grid()

    plt.show()

# plot_train_history(multi_step_history, "lstm")

def create_time_steps(length):
  return list(range(-length, 0))

#plotting function
def multi_step_plot(history, true_future, prediction, title=""):
  plt.figure(figsize=(12, 6))
  num_in = create_time_steps(len(history))
  num_out = len(true_future)
  plt.grid()
  plt.plot(num_in, np.array(history), label='History')
  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',
           label='True Future')
  if prediction.any():
    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',
             label='Predicted Future')
  plt.legend(loc='upper left')
  plt.title(title)
  plt.show()

# partition_name = "validation"
# # y_pred = multi_step_model.predict_generator(dataset_dict[partition_name])

# all_xs = []
# all_ys = []
# for x,y in dataset_dict[partition_name].as_numpy_iterator():
#     all_xs.append(x)
#     all_ys.append(y)
# y_true = np.concatenate(all_ys)
# xs = np.concatenate(all_xs)

# print("shapes", xs.shape, y_true.shape, y_pred.shape)

# for i in range(0, 997,400):
#     multi_step_plot(xs[i, :,-1], y_true[i], y_pred[i], title=f"{partition_name} offset {i}")

# partition_name = "test"
# y_pred = multi_step_model.predict_generator(dataset_dict[partition_name])

# all_xs = []
# all_ys = []
# for x,y in dataset_dict[partition_name].as_numpy_iterator():
#     all_xs.append(x)
#     all_ys.append(y)
# y_true = np.concatenate(all_ys)
# xs = np.concatenate(all_xs)

# print("shapes", xs.shape, y_true.shape, y_pred.shape)

# for i in range(0, 997,300):
#     multi_step_plot(xs[i, :,-1], y_true[i], y_pred[i], title=f"{partition_name} offset {i}")

# output = multi_step_model.predict_generator(dataset_dict[partition_name])

# output.shape

# standard sklearn
def dataset_to_arrays(dataset, flatten=True, last_y=True):
    all_xs = []
    all_ys = []
    for x, y in dataset.as_numpy_iterator():
        all_xs.append(x)
        all_ys.append(y)
    ys = np.concatenate(all_ys)
    xs = np.concatenate(all_xs)
    if flatten:
        xs = xs.reshape(xs.shape[0], -1)
    if last_y:
        ys = ys[:, -1]
    return xs, ys

sklearn_datasets_dict = {
    k:dataset_to_arrays(v)
    for k,v in dataset_dict.items()
}

for k, (xs, ys) in sklearn_datasets_dict.items():
    print(f"{k}:{xs.shape}, {ys.shape}")

# model = ensemble.BaggingRegressor(n_estimators=50)
# model.fit(sklearn_datasets_dict["train"][0], sklearn_datasets_dict["train"][1])

kwargs = dict(
    max_depth=8,
    subsample=0.5,
)

# Each model has to be separate
# lower_model = ensemble.GradientBoostingRegressor(loss="quantile",alpha=0.1,**kwargs)
# mid_model = ensemble.GradientBoostingRegressor(loss="ls", **kwargs)
# upper_model = ensemble.GradientBoostingRegressor(loss="quantile", alpha=0.9, **kwargs)

lower_model = ensemble.GradientBoostingRegressor(loss="quantile",alpha=0.1,**kwargs,)
mid_model = ensemble.GradientBoostingRegressor(loss="ls", **kwargs)
upper_model = ensemble.GradientBoostingRegressor(loss="quantile", alpha=0.9, **kwargs)

lower_model.fit(sklearn_datasets_dict["train"][0], sklearn_datasets_dict["train"][1])
mid_model.fit(sklearn_datasets_dict["train"][0], sklearn_datasets_dict["train"][1])
upper_model.fit(sklearn_datasets_dict["train"][0], sklearn_datasets_dict["train"][1])

result_dict = {}
for partition, (xs, ys) in sklearn_datasets_dict.items():
    y_pred = mid_model.predict(xs)
    print(f"{partition} mae:", metrics.mean_absolute_error(ys, y_pred))
    # multi_step_plot(xs[:, -1], ys, y_pred, title=f"{partition}")

print ("X_means = {}".format(X_means[-1]))
print ("X_devs = {}".format(X_stdev[-1]))

y_pred = (y_pred*X_stdev[-1])+X_means[-1]
print ("y_preds = {}".format(type(y_pred)))
partition_name = "train"

xs, ys = sklearn_datasets_dict[partition_name]
ypred_lb = lower_model.predict(xs)
ypred_mid = mid_model.predict(xs)
ypred_ub = upper_model.predict(xs)

# re scale the predictions
ypred_mid = (ypred_mid*X_stdev[-1])+X_means[-1]
ypred_lb = (ypred_lb*X_stdev[-1])+X_means[-1]
ypred_ub = (ypred_ub*X_stdev[-1])+X_means[-1]
ys = (ys*X_stdev[-1])+X_means[-1]




num_past = xs.shape[0]

num_future = ypred_lb.shape[0]

x_indices_future = np.arange(num_future)
plt.figure(figsize=(20,10))

# plot future predictions
plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)
# convert the time into it, since we want to get the time stamps.
plt.plot(x_indices_future, ypred_mid, "--")
plt.plot(x_indices_future, ys)

plt.title(partition_name)

partition_name = "validation"

xs, ys = sklearn_datasets_dict[partition_name]
ypred_lb = lower_model.predict(xs)
ypred_mid = mid_model.predict(xs)
ypred_ub = upper_model.predict(xs)

# re scale the predictions
ypred_mid = (ypred_mid*X_stdev[-1])+X_means[-1]
ypred_lb = (ypred_lb*X_stdev[-1])+X_means[-1]
ypred_ub = (ypred_ub*X_stdev[-1])+X_means[-1]
ys = (ys*X_stdev[-1])+X_means[-1]

num_past = xs.shape[0]

num_future = ypred_lb.shape[0]

x_indices_future = np.arange(num_future)
plt.figure(figsize=(20,10))

# plot future predictions
plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)
plt.plot(x_indices_future, ypred_mid, "--")
plt.plot(x_indices_future, ys)

plt.title(partition_name)

partition_name = "test"

xs, ys = sklearn_datasets_dict[partition_name]
ypred_lb = lower_model.predict(xs)
ypred_mid = mid_model.predict(xs)
ypred_ub = upper_model.predict(xs)

# re scale the predictions
ypred_mid = (ypred_mid*X_stdev[-1])+X_means[-1]
ypred_lb = (ypred_lb*X_stdev[-1])+X_means[-1]
ypred_ub = (ypred_ub*X_stdev[-1])+X_means[-1]
ys = (ys*X_stdev[-1])+X_means[-1]

num_past = xs.shape[0]

num_future = ypred_lb.shape[0]

x_indices_future = np.arange(num_future)
plt.figure(figsize=(20,10))

# plot future predictions
plt.fill_between(x_indices_future, ypred_lb, ypred_ub, color='b', alpha=0.1)
plt.plot(x_indices_future, ypred_mid, "--")
plt.plot(x_indices_future, ys)

plt.title(partition_name)
plt.show()

# # xgboost
# train mae: 0.09998867638561532
# validation mae: 0.5098038188379816
# test mae: 0.6213932867880211

